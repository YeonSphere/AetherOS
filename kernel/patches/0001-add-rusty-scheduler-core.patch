From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: AetherOS Developer <dev@aetheros.org>
Date: Thu, 21 Nov 2024 09:20:00 +0000
Subject: [PATCH 1/5] sched/core: Add Rusty scheduler core implementation

Add the core implementation of the Rusty scheduler, which provides a novel
scheduling approach focused on improved task scheduling efficiency and
performance.

Signed-off-by: AetherOS Developer <dev@aetheros.org>
---
 kernel/sched/core.c | 193 ++++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 193 insertions(+)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 0000000..1111111 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2210,6 +2210,199 @@ static void
 __do_set_cpus_allowed(struct task_struct *p, struct affinity_context *ctx);
 
+#ifdef CONFIG_SCHED_RUSTY
+#include <linux/sched/rusty.h>
+
+/* Forward declarations */
+static void update_curr_rusty(struct rq *rq);
+static void check_preempt_curr_rusty(struct rq *rq, struct task_struct *p, int flags);
+static struct task_struct *pick_next_task_rusty(struct rq *rq);
+static void put_prev_task_rusty(struct rq *rq, struct task_struct *p);
+static void set_curr_task_rusty(struct rq *rq, struct task_struct *p);
+static void task_tick_rusty(struct rq *rq, struct task_struct *p, int queued);
+static void prio_changed_rusty(struct rq *rq, struct task_struct *p, int oldprio);
+
+/* Rusty scheduler class */
+const struct sched_class rusty_sched_class = {
+    .next = &fair_sched_class,
+    .enqueue_task = enqueue_task_rusty,
+    .dequeue_task = dequeue_task_rusty,
+    .check_preempt_curr = check_preempt_curr_rusty,
+    .pick_next_task = pick_next_task_rusty,
+    .put_prev_task = put_prev_task_rusty,
+    .task_tick = task_tick_rusty,
+    .prio_changed = prio_changed_rusty,
+};
+
+/* Rusty scheduler run queue */
+struct rusty_rq {
+    struct list_head queue;
+    unsigned int nr_running;
+    u64 clock;
+    u64 min_vruntime;
+    raw_spinlock_t lock;
+    unsigned long nr_switches;
+    struct sched_rusty_entity *curr;
+};
+
+static DEFINE_PER_CPU(struct rusty_rq, rusty_rq);
+
+static void enqueue_task_rusty(struct rq *rq, struct task_struct *p, int flags)
+{
+    struct rusty_rq *rusty_rq = &per_cpu(rusty_rq, cpu_of(rq));
+    struct sched_rusty_entity *se = &p->rusty;
+
+    raw_spin_lock(&rusty_rq->lock);
+    update_curr_rusty(rq);
+    list_add_tail(&se->run_list, &rusty_rq->queue);
+    rusty_rq->nr_running++;
+    
+    if (!se->on_rq) {
+        se->on_rq = 1;
+    }
+    raw_spin_unlock(&rusty_rq->lock);
+}
+
+static void dequeue_task_rusty(struct rq *rq, struct task_struct *p, int flags)
+{
+    struct rusty_rq *rusty_rq = &per_cpu(rusty_rq, cpu_of(rq));
+    struct sched_rusty_entity *se = &p->rusty;
+
+    raw_spin_lock(&rusty_rq->lock);
+    update_curr_rusty(rq);
+    list_del_init(&se->run_list);
+    rusty_rq->nr_running--;
+    if (se->on_rq) {
+        se->on_rq = 0;
+    }
+    raw_spin_unlock(&rusty_rq->lock);
+}
+
+static void check_preempt_curr_rusty(struct rq *rq, struct task_struct *p, int flags)
+{
+    struct task_struct *curr = rq->curr;
+    struct sched_rusty_entity *se = &p->rusty;
+    struct sched_rusty_entity *curr_se = &curr->rusty;
+
+    if (entity_before(se, curr_se))
+        resched_curr(rq);
+}
+
+static struct task_struct *pick_next_task_rusty(struct rq *rq)
+{
+    struct rusty_rq *rusty_rq = &per_cpu(rusty_rq, cpu_of(rq));
+    struct sched_rusty_entity *se;
+    struct task_struct *p;
+    raw_spin_lock(&rusty_rq->lock);
+
+    if (!rusty_rq->nr_running) {
+        raw_spin_unlock(&rusty_rq->lock);
+        return NULL;
+    }
+
+    se = pick_next_entity_rusty(rusty_rq);
+    p = container_of(se, struct task_struct, rusty);
+    raw_spin_unlock(&rusty_rq->lock);
+
+    return p;
+}
+
+static void put_prev_task_rusty(struct rq *rq, struct task_struct *p)
+{
+    struct rusty_rq *rusty_rq = &per_cpu(rusty_rq, cpu_of(rq));
+    struct sched_rusty_entity *se = &p->rusty;
+
+    raw_spin_lock(&rusty_rq->lock);
+    if (se->on_rq) {
+        update_curr_rusty(rq);
+        se->exec_start = 0;
+    }
+    if (rq->curr == p) {
+        rq->curr = NULL;
+    }
+    raw_spin_unlock(&rusty_rq->lock);
+}
+
+static void set_curr_task_rusty(struct rq *rq, struct task_struct *p)
+{
+    struct rusty_rq *rusty_rq = &per_cpu(rusty_rq, cpu_of(rq));
+    struct sched_rusty_entity *se = &p->rusty;
+
+    raw_spin_lock(&rusty_rq->lock);
+    if (!se->exec_start) {
+        se->exec_start = rq_clock_task(rq);
+    }
+    if (rq->curr != p) {
+        rusty_rq->curr = se;
+        rq->curr = p;
+    }
+    raw_spin_unlock(&rusty_rq->lock);
+}
+
+static void task_tick_rusty(struct rq *rq, struct task_struct *p, int queued)
+{
+    struct rusty_rq *rusty_rq = &per_cpu(rusty_rq, cpu_of(rq));
+    struct sched_rusty_entity *se = &p->rusty;
+
+    raw_spin_lock(&rusty_rq->lock);
+    update_curr_rusty(rq);
+
+    if (rusty_rq->nr_running > 1 && needs_resched_rusty(se, rusty_rq)) {
+        requeue_task_rusty(rq, p);
+        if (rq->curr == p) {
+            resched_curr(rq);
+        }
+    }
+    raw_spin_unlock(&rusty_rq->lock);
+}
+
+void update_curr_rusty(struct rq *rq)
+{
+    struct rusty_rq *rusty_rq = &per_cpu(rusty_rq, cpu_of(rq));
+    struct task_struct *curr = rq->curr;
+    u64 now = rq_clock_task(rq);
+    raw_spin_lock(&rusty_rq->lock);
+
+    if (curr) {
+        struct sched_rusty_entity *se = &curr->rusty;
+        u64 delta_exec;
+
+        delta_exec = now - curr->se.exec_start;
+        if (unlikely((s64)delta_exec <= 0))
+            delta_exec = 0;
+
+        if (delta_exec > 0) {
+            se->sum_exec_runtime += delta_exec;
+        }
+    }
+    raw_spin_unlock(&rusty_rq->lock);
+}
+
+void init_rusty_rq(struct rusty_rq *rusty_rq)
+{
+    INIT_LIST_HEAD(&rusty_rq->queue);
+    rusty_rq->nr_running = 0;
+    rusty_rq->clock = 0;
+    rusty_rq->min_vruntime = 0;
+    raw_spin_lock_init(&rusty_rq->lock);
+    rusty_rq->curr = NULL;
+}
+
+static int __init rusty_init(void)
+{
+    int cpu;
+
+    for_each_possible_cpu(cpu) {
+        struct rusty_rq *rusty_rq = &per_cpu(rusty_rq, cpu);
+        init_rusty_rq(rusty_rq);
+    }
+
+    return 0;
+}
+early_initcall(rusty_init);
+
+#endif /* CONFIG_SCHED_RUSTY */
+
 #ifdef CONFIG_SMP
 
 /*
-- 
2.34.1
