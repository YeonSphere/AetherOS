diff --git a/kernel/sched/rusty_lb.c b/kernel/sched/rusty_lb.c
new file mode 100644
index 0000000..1234567
--- /dev/null
+++ b/kernel/sched/rusty_lb.c
@@ -0,0 +1,187 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#include <linux/sched.h>
+#include <linux/sched/clock.h>
+#include <linux/init_task.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/cpumask.h>
+#include <linux/smp.h>
+#include <linux/preempt.h>
+#include <linux/sched/rt.h>
+
+#include <linux/sched/rusty.h>
+
+#ifdef CONFIG_SCHED_RUSTY
+
+/* Load balancing interval */
+#define RUSTY_LB_INTERVAL (HZ/10)
+
+/* Load imbalance threshold */
+#define RUSTY_IMBALANCE_PCT 125
+
+struct rusty_lb_env {
+    struct rusty_rq *src_rq;
+    struct rusty_rq *dst_rq;
+    struct task_struct *push_task;
+    int push_cpu;
+    int pull_cpu;
+    int flags;
+};
+
+static inline int rusty_can_migrate_task(struct task_struct *p, struct rusty_lb_env *env)
+{
+    struct rusty_rq *src_rq = env->src_rq;
+    struct rusty_rq *dst_rq = env->dst_rq;
+
+    /* Don't migrate running tasks */
+    if (task_running(src_rq->rq, p))
+        return 0;
+
+    /* Check if task is allowed to run on destination CPU */
+    if (!cpumask_test_cpu(env->dst_rq->rq->cpu, &p->cpus_mask))
+        return 0;
+
+    /* Check load balancing conditions */
+    if (src_rq->nr_running <= 1)
+        return 0;
+
+    return 1;
+}
+
+static struct task_struct *rusty_pick_next_pushable_task(struct rusty_rq *rq)
+{
+    struct task_struct *p;
+    struct sched_rusty_entity *se;
+
+    if (!rq->nr_running || rq->nr_running == 1)
+        return NULL;
+
+    list_for_each_entry(se, &rq->queue, run_list) {
+        p = container_of(se, struct task_struct, rusty);
+        if (task_running(rq->rq, p))
+            continue;
+
+        return p;
+    }
+
+    return NULL;
+}
+
+static int rusty_active_load_balance_cpu_stop(void *data)
+{
+    struct rusty_lb_env *env = data;
+    struct rusty_rq *src_rq = env->src_rq, *dst_rq = env->dst_rq;
+    struct task_struct *p = NULL;
+    struct sched_rusty_entity *se;
+
+    raw_spin_lock_irq(&src_rq->lock);
+
+    p = rusty_pick_next_pushable_task(src_rq);
+    if (!p)
+        goto unlock;
+
+    if (!rusty_can_migrate_task(p, env))
+        goto unlock;
+
+    se = &p->rusty;
+    deactivate_task(src_rq->rq, p, 0);
+    set_task_cpu(p, dst_rq->rq->cpu);
+    activate_task(dst_rq->rq, p, 0);
+
+unlock:
+    raw_spin_unlock_irq(&src_rq->lock);
+    return 0;
+}
+
+static int rusty_load_balance(int this_cpu, struct rq *this_rq,
+                           struct sched_domain *sd, enum cpu_idle_type idle,
+                           int *continue_balancing)
+{
+    struct rusty_rq *src_rq, *dst_rq;
+    struct rusty_lb_env env = {
+        .flags = 0,
+    };
+    struct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);
+    int cpu;
+
+    cpumask_copy(cpus, sched_domain_span(sd));
+    cpumask_clear_cpu(this_cpu, cpus);
+
+    if (cpumask_empty(cpus))
+        return 0;
+
+    dst_rq = &per_cpu(rusty_rq, this_cpu);
+
+    /* Find busiest CPU in domain */
+    for_each_cpu(cpu, cpus) {
+        src_rq = &per_cpu(rusty_rq, cpu);
+        if (src_rq->nr_running > dst_rq->nr_running + 1) {
+            env.src_rq = src_rq;
+            env.dst_rq = dst_rq;
+            env.push_cpu = cpu;
+            env.pull_cpu = this_cpu;
+            break;
+        }
+    }
+
+    if (!env.src_rq)
+        return 0;
+
+    /* Perform load balancing */
+    stop_one_cpu_nowait(cpu, rusty_active_load_balance_cpu_stop,
+                        &env, per_cpu_ptr(&rusty_lb_work, cpu));
+
+    return 1;
+}
+
+static void rusty_load_balance_newidle(struct rq *this_rq, struct rq_flags *rf)
+{
+    int this_cpu = this_rq->cpu;
+    struct sched_domain *sd;
+    int continue_balancing = 1;
+
+    for_each_domain(this_cpu, sd) {
+        rusty_load_balance(this_cpu, this_rq, sd,
+                         CPU_NEWLY_IDLE, &continue_balancing);
+        if (!continue_balancing)
+            break;
+    }
+}
+
+void rusty_trigger_load_balance(struct rq *rq)
+{
+    if (time_after_eq(jiffies, rq->next_balance))
+        raise_softirq(SCHED_SOFTIRQ);
+}
+
+#endif /* CONFIG_SCHED_RUSTY */
